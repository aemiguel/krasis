================================================================
Krasis Benchmark — 2026-02-15 12:29:35
================================================================
Model:            Qwen3-Coder-Next
Architecture:     qwen3_next, 48 layers, 512 experts, top-10, 12 GQA + 36 linear
PP Partition:     [24, 24] (2 GPUs)

Hardware:
  CPU:            AMD EPYC 7742 64-Core Processor (64 cores)
  RAM:            995 GB total, 76.1 GB used by process
  GPU 0:          NVIDIA RTX 2000 Ada Generation (16380 MB), 6389 MB allocated
  GPU 1:          NVIDIA RTX 2000 Ada Generation (16380 MB), 6240 MB allocated
  GPU 2:          NVIDIA RTX 2000 Ada Generation (16380 MB), 0 MB allocated

Quantization:
  GPU experts:    INT4 (Marlin)
  CPU experts:    INT4
  Attention:      INT8
  Shared expert:  INT8
  Dense MLP:      INT8
  LM head:        INT8
  KV cache:       FP8 E4M3

Strategy:
  Expert divisor: 4 (layer_grouped(4))
  Prefill threshold: 300
  Mode:           pure_cpu decode, layer_grouped(4) prefill

Prefill (10000 tokens, 3 runs):
  Run 1:  461.6 tok/s, TTFT=21.66s
  Run 2:  587.8 tok/s, TTFT=17.01s
  Run 3:  587.6 tok/s, TTFT=17.02s
  Average: 545.7 tok/s, TTFT=18.56s

Decode (64 tokens, 3 runs):
  Run 1:  10.05 tok/s (99.5ms/tok)
  Run 2:  9.99 tok/s (100.1ms/tok)
  Run 3:  10.06 tok/s (99.4ms/tok)
  Average: 10.03 tok/s (99.7ms/tok)

Verification:
  Prefill prompt: user
Explain distributed consensus algorithms including Paxos, Raft, and PBFT. Describe database transaction isolation levels and their trade-offs. Discuss compiler optimization passes such as dead co... [63136 chars total]
  Decode prompt:  Write a poem about recursion in programming.
  Generated output (64 tokens):
    **The Labyrinth of Self-Reference**  
    
    A function stands, both tall and thin,  
    With purpose sharp and purpose thin—  
    To solve a task, to break it down,  
    To fold the world in self-refined crown.  
    
    *“Call me again,”* it softly says,  
    *“
================================================================
